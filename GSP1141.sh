#!/bin/bash
set -e

echo "=============================="
echo "üìÑ Document AI Automated Setup"
echo "=============================="

# üîß Config
PROJECT_ID=$(gcloud config get-value project)
LOCATION="${1:-us}"  # Default to "us" if no arg passed
DISPLAY_NAME="lab-invoice-uptraining"
PROCESSOR_TYPE="INVOICE_PROCESSOR"
BUCKET_NAME="${PROJECT_ID}-uptraining-lab"
SAMPLE_GCS_PATH="gs://cloud-samples-data/documentai/codelabs/uptraining/pdfs"
DEST_PATH="pdfs"  # Folder inside your bucket

echo "üß© Using Project: $PROJECT_ID"
echo "üìç Location: $LOCATION"
echo "üßæ Processor Display Name: $DISPLAY_NAME"
echo "ü™£ GCS Bucket: $BUCKET_NAME"

# 1Ô∏è‚É£ Enable Document AI API
echo "1‚É£ Enabling Document AI API..."
gcloud services enable documentai.googleapis.com

# 2Ô∏è‚É£ Install Python Client Library
echo "2‚É£ Installing Document AI Python client..."
pip3 install --upgrade google-cloud-documentai

# 3Ô∏è‚É£ Create Processor
echo "3‚É£ Creating Processor..."
cat <<EOF > create_proc.json
{
  "type": "${PROCESSOR_TYPE}",
  "displayName": "${DISPLAY_NAME}"
}
EOF

PROC_RESPONSE=$(curl -s -X POST \
  -H "Authorization: Bearer $(gcloud auth print-access-token)" \
  -H "Content-Type: application/json" \
  -d @create_proc.json \
  "https://${LOCATION}-documentai.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/processors")

PROCESSOR_NAME=$(echo "$PROC_RESPONSE" | grep -o '"name"[ ]*:[ ]*"[^"]*' | cut -d'"' -f4)
PROCESSOR_ID=$(basename "$PROCESSOR_NAME")

echo "‚úÖ Processor Created: $PROCESSOR_ID"

# 4Ô∏è‚É£ Create GCS Bucket (if not exists)
echo "4‚É£ Creating GCS Bucket (if needed)..."
if ! gsutil ls -b "gs://${BUCKET_NAME}" &>/dev/null; then
  gcloud storage buckets create "gs://${BUCKET_NAME}" --location="${LOCATION}"
  echo "‚úÖ Bucket created: gs://${BUCKET_NAME}"
else
  echo "‚ÑπÔ∏è Bucket already exists: gs://${BUCKET_NAME}"
fi

# 5Ô∏è‚É£ Create Dataset for Processor
echo "5‚É£ Creating Dataset in Custom GCS Bucket..."
cat <<EOF > create_dataset.json
{
  "name":"projects/${PROJECT_ID}/locations/${LOCATION}/processors/${PROCESSOR_ID}/dataset",
  "gcs_managed_config": {
    "gcs_prefix": {
      "gcs_uri_prefix": "gs://${BUCKET_NAME}"
    }
  },
  "spanner_indexing_config": {}
}
EOF

curl -s -X PATCH \
  -H "Authorization: Bearer $(gcloud auth print-access-token)" \
  -H "Content-Type: application/json" \
  -d @create_dataset.json \
  "https://${LOCATION}-documentai.googleapis.com/v1beta3/projects/${PROJECT_ID}/locations/${LOCATION}/processors/${PROCESSOR_ID}/dataset"

echo "‚úÖ Dataset configured."

# 6Ô∏è‚É£ Copy Sample Documents to User Bucket
echo "6‚É£ Copying sample documents into your GCS bucket..."
gsutil -m cp -r "${SAMPLE_GCS_PATH}" "gs://${BUCKET_NAME}/${DEST_PATH}/"
echo "‚úÖ Sample documents copied to: gs://${BUCKET_NAME}/${DEST_PATH}/"

echo "=============================="
echo "‚úÖ steps 3 completed!"
echo "üßæ Processor ID: $PROCESSOR_ID"
echo "üóÇ Sample documents path: gs://${BUCKET_NAME}/${DEST_PATH}/"
echo "üïí You can monitor the import operation in the Document AI Console or via:"
echo "   gcloud ai operations describe $OP_NAME --location=$LOCATION"
